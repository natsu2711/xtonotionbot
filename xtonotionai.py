# -*- coding: utf-8 -*-
import os
import time
import asyncio
import random
import re
from typing import Union
from playwright.async_api import async_playwright, TimeoutError
from notion_client import Client
from dotenv import load_dotenv



# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# --- é…ç½®åŒº ---
NOTION_API_KEY = os.getenv("NOTION_API_KEY")
NOTION_DATABASE_ID = os.getenv("NOTION_DATABASE_ID")
PROCESSED_TWEETS = set()

# === æ ¸å¿ƒé…ç½® ===
LIKE_THRESHOLD = 800
REPOST_THRESHOLD = 50
LIKE_PROBABILITY = 0.6
TOTAL_SCROLLS = 20 
CUSTOM_EXPLAIN_PROMPT = "è¯·å¯¹ä»¥ä¸‹æ¨æ–‡è¿›è¡Œæ·±åº¦å‰–æï¼Œè¾“å‡ºæ€»é•¿åº¦ä¸å¾—è¶…è¿‡2000å­—,ç”¨ä¸­æ–‡å›ç­”ã€‚å›ç­”éœ€ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„ï¼š1. æ‘˜è¦ï¼šç”¨300å­—ä»¥å†…é«˜åº¦æ¦‚æ‹¬æ¨æ–‡çš„æœ¬è´¨è§‚ç‚¹å’Œæ ¸å¿ƒçŸ›ç›¾ã€‚2. æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼ˆ3â€“5æ¡ï¼‰ï¼šåªä¿ç•™ä¸æ¨æ–‡ä¸»é¢˜æœ€ç´§å¯†ç›¸å…³çš„çŸ¥è¯†ç‚¹ï¼Œæ¯æ¡â‰¤100å­—ã€‚3. æ ¸å¿ƒé—®é¢˜ï¼ˆ2â€“4æ¡ï¼‰ï¼šåˆ—å‡ºæ¨æ–‡èƒŒåçœŸæ­£è¦è§£å†³çš„å…³é”®çŸ›ç›¾ï¼Œæ¯æ¡â‰¤100å­—ã€‚4. ä¸šåŠ¡è¿ä½œå…¨æµç¨‹ï¼ˆ4â€“6æ­¥ï¼‰ï¼šç”¨ç®€æ˜æ­¥éª¤å±•ç¤ºè¯¥é—®é¢˜åœ¨çœŸå®æƒ…å¢ƒä¸­çš„è¿è¡Œé€»è¾‘ã€‚ç„¶åï¼Œä»…é€‰æ‹©æœ€é‡è¦çš„2ä¸ªçŸ¥è¯†ç‚¹å’Œ2ä¸ªæ ¸å¿ƒé—®é¢˜ï¼Œé€ä¸€è¿›è¡Œâ€œå››å±‚æ¬¡æ·±åº¦å‰–æï¼šç¬¬ä¸€æ€§åŸç†ï¼šæŒ‡å‡ºè¯¥é¢†åŸŸç¨³å®šä¸å˜çš„é€šç”¨æ³•åˆ™ï¼Œè§£é‡Šå…¶çŸ›ç›¾ä¸ç›²ç‚¹ï¼Œå¯å¼•ç”¨ç»å…¸ä¹¦ç±/è®ºæ–‡ã€‚æ¨¡å‹ï¼šç»™å‡ºåŸºäºåŸç†çš„ç»“æ„åŒ–æ¡†æ¶æˆ–ç®—æ³•ï¼ˆç®€åŒ–ã€å¯åº¦é‡ã€å¯æ¼”ç®—ï¼‰ã€‚æ“ä½œï¼šå†™å‡º4â€“6æ­¥çš„æµç¨‹åŒ–å®æ–½æ–¹æ¡ˆï¼Œè§£é‡Šæ¯æ­¥çš„åŸå› ï¼Œå¹¶ç”¨è´´åˆ‡ç±»æ¯”å¸®åŠ©ç†è§£ã€‚ç»éªŒï¼šæä¾›ä¸€ä¸ªçœŸå®æ¡ˆä¾‹æˆ–æ•…äº‹ï¼Œå±•ç¤ºæ‰§è¡Œåçš„åé¦ˆä¸å¤ç›˜ã€‚è¦æ±‚ï¼š  å…¨æ–‡â‰¤2000å­—ï¼Œå¿…è¦æ—¶ä¼˜å…ˆä¿ç•™â€œæ‘˜è¦ > æ ¸å¿ƒé—®é¢˜ > ç¬¬ä¸€æ€§åŸç† > æ¨¡å‹â€ã€‚  ä½¿ç”¨çº¯æ–‡æœ¬è¾“å‡ºï¼Œä¸å…è®¸å‡ºç°ä»»ä½• Markdown æ ¼å¼ç¬¦å·ï¼ˆä¾‹å¦‚ ###ã€**ã€-ã€â€¢ï¼‰ã€‚é¿å…èµ˜è¿°ä¸é‡å¤ï¼Œä¸è¦è¾“å‡ºå®¢å¥—è¯ï¼Œä¸è¦å¤šä½™å…ƒä¿¡æ¯ã€‚è¾“å‡ºé£æ ¼è¦ç®€æ´å‡ç»ƒï¼Œä½†æ€æƒ³æ·±åˆ»ã€‚"
DAILY_GOAL = 5

# --- è¾…åŠ©å‡½æ•°ï¼šè§£ææ•°å­— ---
def parse_count(text: str) -> int:
    if not text: return 0
    text_upper = text.upper()
    match = re.search(r'([\d.,]+)\s?([KM]?)', text_upper)
    if not match: return 0
    number_str = match.group(1).replace(',', '')
    modifier = match.group(2)
    try:
        value = float(number_str)
        if modifier == 'K': return int(value * 1000)
        elif modifier == 'M': return int(value * 1000000)
        else: return int(value)
    except (ValueError, TypeError): return 0

# === æ›´æ–°ï¼šadd_to_notion å‡½æ•°ï¼Œå¢åŠ æ™ºèƒ½æˆªæ–­åŠŸèƒ½ ===
async def add_to_notion(summary_text, tweet_url, original_tweet_text):
    """æ ¹æ®æ‚¨çš„æ•°æ®åº“å­—æ®µ (Name, URL, ç®€ä»‹) å°†å†…å®¹æ·»åŠ åˆ°Notion"""
    try:
        query_results = notion.databases.query(database_id=NOTION_DATABASE_ID, filter={"property": "URL", "url": {"equals": tweet_url}})
        if query_results["results"]:
            print(f"    - Skipped Notion: Tweet already exists. URL: {tweet_url}")
            return False
        
        # å‡†å¤‡ç®€ä»‹å†…å®¹
        summary_content = summary_text or "No summary generated."
        
        # æ™ºèƒ½æˆªæ–­ï¼šå¦‚æœå†…å®¹è¶…è¿‡2000å­—ç¬¦ï¼Œåˆ™æˆªå–å¹¶æ·»åŠ çœç•¥å·
        if len(summary_content) > 2000:
            summary_content = summary_content[:1995] + "..."
            print("    - INFO: Summary was truncated to fit Notion's 2000 character limit.")

        properties = {
            "ç®€ä»‹": {"rich_text": [{"text": {"content": summary_content}}]},
            "URL": {"url": tweet_url},
            "Name": {"title": [{"text": {"content": original_tweet_text or "Untitled Tweet"}}]}
        }
        
        notion.pages.create(parent={"database_id": NOTION_DATABASE_ID}, properties=properties)
        print(f"    - âœ… SUCCESS: Added generated summary to Notion.")
        return True
    except Exception as e:
        print(f"    - âŒ ERROR: Failed to add to Notion. Reason: {e}")
        return False

async def get_summary_from_grok_site(context, tweet_url) -> Union[str , None]:
    """æ‰“å¼€grok.comï¼Œç²˜è´´é“¾æ¥å’Œæç¤ºè¯ï¼Œå¹¶ç²¾ç¡®åœ°å¤åˆ¶AIçš„å›ç­”"""
    grok_page = None
    try:
        print("    - Navigating to grok.com...")
        grok_page = await context.new_page()
        await grok_page.goto("https://grok.com/", wait_until="domcontentloaded", timeout=60000)

        prompt_input = grok_page.locator("textarea[aria-label='å‘ Grok æä»»ä½•é—®é¢˜']")
        await prompt_input.wait_for(state="visible", timeout=60000)
        
        full_prompt = f"è¿™æ˜¯æ¨æ–‡é“¾æ¥: {tweet_url}\n\n{CUSTOM_EXPLAIN_PROMPT}"
        await prompt_input.fill(full_prompt)
        
        submit_button = grok_page.locator("button[aria-label='æäº¤']")
        await submit_button.click()
        print("    - ACTION: Submitted prompt to grok.com. Waiting for response...")

        stop_button = grok_page.locator("button[aria-label='åœæ­¢æ¨¡å‹å“åº”']")
        await stop_button.wait_for(state="hidden", timeout=300000)
        await asyncio.sleep(2)  # ç»™ DOM æ¸²æŸ“ç•™æ—¶é—´
        print("    - INFO: AI response is complete.")
        await asyncio.sleep(1)

        last_response_container = grok_page.locator(".last-response")
        copy_button = last_response_container.locator("button[aria-label='å¤åˆ¶']")
        await copy_button.wait_for(state="visible", timeout=300000)
        await copy_button.click()
        print("    - ACTION: Clicked the AI's specific 'Copy' button.")
        await asyncio.sleep(0.5)

        generated_summary = await grok_page.evaluate("() => navigator.clipboard.readText()")
        print("    - âœ… SUCCESS: Successfully read AI summary from clipboard.")

        return generated_summary
    except Exception as e:
        print(f"    - âŒ ERROR on grok.com: {type(e).__name__} - {e}")
        return None
    finally:
        if grok_page:
            await grok_page.close()
            print("    - ACTION: Closed grok.com tab.")


async def scrape_main_timeline():
    """ä¸»å‡½æ•°ï¼Œé‡‡ç”¨å®æ—¶æµå¤„ç†é€»è¾‘"""
    processed_count = 0
    async with async_playwright() as p:
        browser = None # åœ¨tryå—å¤–éƒ¨å®šä¹‰browser
        context = None # åœ¨tryå—å¤–éƒ¨å®šä¹‰context
        try:
            # --- äº‘ç«¯å¯åŠ¨é€»è¾‘ ---
            browser = await p.chromium.launch(headless=True)
            
            # å°è¯•ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ "è®°å¿†èƒ¶å›Š"
            try:
                context = await browser.new_context(storage_state="storage_state.json")
                print("âœ… Launched new browser instance in cloud and loaded existing storage_state.json.")
            except FileNotFoundError:
                print("âš ï¸ storage_state.json not found. Creating a new context. A new state will be saved at the end.")
                context = await browser.new_context()

            page = await context.new_page()
            
        except Exception as e:
            print(f"âŒ FATAL ERROR during browser setup in cloud: {type(e).__name__} - {e}")
            if browser:
                await browser.close()
            return


        await page.goto("https://x.com", wait_until="domcontentloaded", timeout=60000)
        print(f"--- Navigated to main timeline. Goal: {DAILY_GOAL} summaries. ---")

        for i in range(TOTAL_SCROLLS):
            if processed_count >= DAILY_GOAL:
                print(f"\nğŸ‰ Daily goal of {DAILY_GOAL} summaries reached. Halting script.")
                break

            print(f"\n--- Scrolling... Round {i+1}/{TOTAL_SCROLLS} ---")
            await page.mouse.wheel(0, 8000)
            await asyncio.sleep(4)

            articles = page.locator('article[data-testid="tweet"]')
            count = await articles.count()
            print(f"  - Found {count} potential tweets on the page.")

            for i in range(count):
                if processed_count >= DAILY_GOAL: break
                
                article = articles.nth(i)
                try:
                    link_locator = article.locator("a[href*='/status/']").first
                    href = await link_locator.get_attribute('href')
                    if not href: continue
                    tweet_url = "https://x.com" + href

                    if tweet_url in PROCESSED_TWEETS: continue
                    
                    button_group = article.locator("div[role='group']")
                    like_locator = button_group.locator("button[aria-label*='Like']")
                    repost_locator = button_group.locator("button[aria-label*='Repost']")
                    like_text = await like_locator.inner_text() if await like_locator.count() > 0 else ""
                    repost_text = await repost_locator.inner_text() if await repost_locator.count() > 0 else ""
                    like_count = parse_count(like_text)
                    repost_count = parse_count(repost_text)

                    if like_count > LIKE_THRESHOLD or repost_count > REPOST_THRESHOLD:
                        print(f"\n  -> Found high-value tweet: {tweet_url} (Likes: {like_count}, Reposts: {repost_count})")
                        PROCESSED_TWEETS.add(tweet_url)

                        text_locator = article.locator('[data-testid="tweetText"]')
                        original_tweet_text = await text_locator.inner_text() if await text_locator.count() > 0 else tweet_url
                        
                        # --- æ ¸å¿ƒé€»è¾‘å˜æ›´ï¼šç«‹å³æ‰§è¡Œéšæœºç‚¹èµ ---
                        if random.random() < LIKE_PROBABILITY:
                            if await like_locator.is_visible() and "Unlike" not in (await like_locator.get_attribute("aria-label")):
                                    await like_locator.click()
                                    print("  - âœ… ACTION: Liked tweet to train algorithm.")
                                    await asyncio.sleep(random.uniform(1, 3))
                            else: print("  - INFO: Tweet was already liked.")
                        else: print("  - INFO: Skipped liking due to random chance.")
                        
                        # --- ç„¶åå†å»æ‰§è¡Œè€—æ—¶çš„AIæ‘˜è¦ä»»åŠ¡ ---
                        summary_text = await get_summary_from_grok_site(context, tweet_url)
                        
                        if summary_text:
                            is_added = await add_to_notion(summary_text, tweet_url, original_tweet_text)
                            if is_added:
                                processed_count += 1
                                print(f"  - PROGRESS: {processed_count}/{DAILY_GOAL} summaries collected.")
                        else:
                            print("  - INFO: Failed to get summary from grok.com, moving to next tweet.")
                except Exception as e:
                    continue
        
        finally:
            # --- ç¡®ä¿æœ€åä¿å­˜çŠ¶æ€å¹¶å…³é—­æµè§ˆå™¨ ---
            if context:
                print("\n--- Saving updated storage state... ---")
                await context.storage_state(path="storage_state.json")
                print("--- Updated storage state saved to storage_state.json. ---")
            if browser:
                await browser.close()
                print("--- Browser closed. ---")
    
    print(f"\n--- Script finished. Total summaries collected: {processed_count} ---")

if __name__ == "__main__":
    notion = Client(auth=NOTION_API_KEY)
    asyncio.run(scrape_main_timeline())