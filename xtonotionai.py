# -*- coding: utf-8 -*-
import os
import time
import asyncio
import random
import re
from typing import Union
from playwright.async_api import async_playwright, TimeoutError
from notion_client import Client
from dotenv import load_dotenv



# åŠ è½½ .env æ–‡ä»¶ä¸­çš„çŽ¯å¢ƒå˜é‡
load_dotenv()

# --- é…ç½®åŒº ---
NOTION_API_KEY = os.getenv("NOTION_API_KEY")
NOTION_DATABASE_ID = os.getenv("NOTION_DATABASE_ID")
PROCESSED_TWEETS = set()

# === æ ¸å¿ƒé…ç½® ===
LIKE_THRESHOLD = 10
REPOST_THRESHOLD = 5
LIKE_PROBABILITY = 0.6
TOTAL_SCROLLS = 20 
CUSTOM_EXPLAIN_PROMPT = "è¯·å¯¹ä»¥ä¸‹æŽ¨æ–‡è¿›è¡Œæ·±åº¦å‰–æžï¼Œè¾“å‡ºæ€»é•¿åº¦ä¸å¾—è¶…è¿‡2000å­—,ç”¨ä¸­æ–‡å›žç­”ã€‚å›žç­”éœ€ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æž„ï¼š1. æ‘˜è¦ï¼šç”¨300å­—ä»¥å†…é«˜åº¦æ¦‚æ‹¬æŽ¨æ–‡çš„æœ¬è´¨è§‚ç‚¹å’Œæ ¸å¿ƒçŸ›ç›¾ã€‚2. æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼ˆ3â€“5æ¡ï¼‰ï¼šåªä¿ç•™ä¸ŽæŽ¨æ–‡ä¸»é¢˜æœ€ç´§å¯†ç›¸å…³çš„çŸ¥è¯†ç‚¹ï¼Œæ¯æ¡â‰¤100å­—ã€‚3. æ ¸å¿ƒé—®é¢˜ï¼ˆ2â€“4æ¡ï¼‰ï¼šåˆ—å‡ºæŽ¨æ–‡èƒŒåŽçœŸæ­£è¦è§£å†³çš„å…³é”®çŸ›ç›¾ï¼Œæ¯æ¡â‰¤100å­—ã€‚4. ä¸šåŠ¡è¿ä½œå…¨æµç¨‹ï¼ˆ4â€“6æ­¥ï¼‰ï¼šç”¨ç®€æ˜Žæ­¥éª¤å±•ç¤ºè¯¥é—®é¢˜åœ¨çœŸå®žæƒ…å¢ƒä¸­çš„è¿è¡Œé€»è¾‘ã€‚ç„¶åŽï¼Œä»…é€‰æ‹©æœ€é‡è¦çš„2ä¸ªçŸ¥è¯†ç‚¹å’Œ2ä¸ªæ ¸å¿ƒé—®é¢˜ï¼Œé€ä¸€è¿›è¡Œâ€œå››å±‚æ¬¡æ·±åº¦å‰–æžï¼šç¬¬ä¸€æ€§åŽŸç†ï¼šæŒ‡å‡ºè¯¥é¢†åŸŸç¨³å®šä¸å˜çš„é€šç”¨æ³•åˆ™ï¼Œè§£é‡Šå…¶çŸ›ç›¾ä¸Žç›²ç‚¹ï¼Œå¯å¼•ç”¨ç»å…¸ä¹¦ç±/è®ºæ–‡ã€‚æ¨¡åž‹ï¼šç»™å‡ºåŸºäºŽåŽŸç†çš„ç»“æž„åŒ–æ¡†æž¶æˆ–ç®—æ³•ï¼ˆç®€åŒ–ã€å¯åº¦é‡ã€å¯æ¼”ç®—ï¼‰ã€‚æ“ä½œï¼šå†™å‡º4â€“6æ­¥çš„æµç¨‹åŒ–å®žæ–½æ–¹æ¡ˆï¼Œè§£é‡Šæ¯æ­¥çš„åŽŸå› ï¼Œå¹¶ç”¨è´´åˆ‡ç±»æ¯”å¸®åŠ©ç†è§£ã€‚ç»éªŒï¼šæä¾›ä¸€ä¸ªçœŸå®žæ¡ˆä¾‹æˆ–æ•…äº‹ï¼Œå±•ç¤ºæ‰§è¡ŒåŽçš„åé¦ˆä¸Žå¤ç›˜ã€‚è¦æ±‚ï¼š  å…¨æ–‡â‰¤2000å­—ï¼Œå¿…è¦æ—¶ä¼˜å…ˆä¿ç•™â€œæ‘˜è¦ > æ ¸å¿ƒé—®é¢˜ > ç¬¬ä¸€æ€§åŽŸç† > æ¨¡åž‹â€ã€‚  ä½¿ç”¨çº¯æ–‡æœ¬è¾“å‡ºï¼Œä¸å…è®¸å‡ºçŽ°ä»»ä½• Markdown æ ¼å¼ç¬¦å·ï¼ˆä¾‹å¦‚ ###ã€**ã€-ã€â€¢ï¼‰ã€‚é¿å…èµ˜è¿°ä¸Žé‡å¤ï¼Œä¸è¦è¾“å‡ºå®¢å¥—è¯ï¼Œä¸è¦å¤šä½™å…ƒä¿¡æ¯ã€‚è¾“å‡ºé£Žæ ¼è¦ç®€æ´å‡ç»ƒï¼Œä½†æ€æƒ³æ·±åˆ»ã€‚"
DAILY_GOAL = 5

# --- è¾…åŠ©å‡½æ•°ï¼šè§£æžæ•°å­— ---
def parse_count(text: str) -> int:
    if not text: return 0
    text_upper = text.upper()
    match = re.search(r'([\d.,]+)\s?([KM]?)', text_upper)
    if not match: return 0
    number_str = match.group(1).replace(',', '')
    modifier = match.group(2)
    try:
        value = float(number_str)
        if modifier == 'K': return int(value * 1000)
        elif modifier == 'M': return int(value * 1000000)
        else: return int(value)
    except (ValueError, TypeError): return 0

# === æ›´æ–°ï¼šadd_to_notion å‡½æ•°ï¼Œå¢žåŠ æ™ºèƒ½æˆªæ–­åŠŸèƒ½ ===
async def add_to_notion(summary_text, tweet_url, original_tweet_text):
    """æ ¹æ®æ‚¨çš„æ•°æ®åº“å­—æ®µ (Name, URL, ç®€ä»‹) å°†å†…å®¹æ·»åŠ åˆ°Notion"""
    try:
        query_results = notion.databases.query(database_id=NOTION_DATABASE_ID, filter={"property": "URL", "url": {"equals": tweet_url}})
        if query_results["results"]:
            print(f"    - Skipped Notion: Tweet already exists. URL: {tweet_url}")
            return False
        
        # å‡†å¤‡ç®€ä»‹å†…å®¹
        summary_content = summary_text or "No summary generated."
        
        # æ™ºèƒ½æˆªæ–­ï¼šå¦‚æžœå†…å®¹è¶…è¿‡2000å­—ç¬¦ï¼Œåˆ™æˆªå–å¹¶æ·»åŠ çœç•¥å·
        if len(summary_content) > 2000:
            summary_content = summary_content[:1995] + "..."
            print("    - INFO: Summary was truncated to fit Notion's 2000 character limit.")

        properties = {
            "ç®€ä»‹": {"rich_text": [{"text": {"content": summary_content}}]},
            "URL": {"url": tweet_url},
            "Name": {"title": [{"text": {"content": original_tweet_text or "Untitled Tweet"}}]}
        }
        
        notion.pages.create(parent={"database_id": NOTION_DATABASE_ID}, properties=properties)
        print(f"    - âœ… SUCCESS: Added generated summary to Notion.")
        return True
    except Exception as e:
        print(f"    - âŒ ERROR: Failed to add to Notion. Reason: {e}")
        return False

async def get_summary_from_grok_site(context, tweet_url) -> Union[str , None]:
    """æ‰“å¼€grok.comï¼Œç²˜è´´é“¾æŽ¥å’Œæç¤ºè¯ï¼Œå¹¶ç²¾ç¡®åœ°å¤åˆ¶AIçš„å›žç­”"""
    grok_page = None
    try:
        print("    - Navigating to grok.com...")
        grok_page = await context.new_page()
        await grok_page.goto("https://grok.com/", wait_until="domcontentloaded", timeout=60000)

        prompt_input = grok_page.locator("textarea[aria-label='å‘ Grok æä»»ä½•é—®é¢˜']")
        await prompt_input.wait_for(state="visible", timeout=60000)
        
        full_prompt = f"è¿™æ˜¯æŽ¨æ–‡é“¾æŽ¥: {tweet_url}\n\n{CUSTOM_EXPLAIN_PROMPT}"
        await prompt_input.fill(full_prompt)
        
        submit_button = grok_page.locator("button[aria-label='æäº¤']")
        await submit_button.click()
        print("    - ACTION: Submitted prompt to grok.com. Waiting for response...")

        stop_button = grok_page.locator("button[aria-label='åœæ­¢æ¨¡åž‹å“åº”']")
        await stop_button.wait_for(state="hidden", timeout=300000)
        await asyncio.sleep(2)  # ç»™ DOM æ¸²æŸ“ç•™æ—¶é—´
        print("    - INFO: AI response is complete.")
        await asyncio.sleep(1)

        last_response_container = grok_page.locator(".last-response")
        copy_button = last_response_container.locator("button[aria-label='å¤åˆ¶']")
        await copy_button.wait_for(state="visible", timeout=300000)
        await copy_button.click()
        print("    - ACTION: Clicked the AI's specific 'Copy' button.")
        await asyncio.sleep(0.5)

        generated_summary = await grok_page.evaluate("() => navigator.clipboard.readText()")
        print("    - âœ… SUCCESS: Successfully read AI summary from clipboard.")

        return generated_summary
    except Exception as e:
        print(f"    - âŒ ERROR on grok.com: {type(e).__name__} - {e}")
        return None
    finally:
        if grok_page:
            await grok_page.close()
            print("    - ACTION: Closed grok.com tab.")


# === ä¿®æ­£è¯­æ³•çš„ scrape_main_timeline å‡½æ•° ===
async def scrape_main_timeline():
    """ä¸»å‡½æ•°ï¼Œé€‚é…äº‘ç«¯çŽ¯å¢ƒï¼Œé‡‡ç”¨å®žæ—¶æµå¤„ç†é€»è¾‘ï¼Œå¹¶æŒä¹…åŒ–ç™»å½•çŠ¶æ€"""
    processed_count = 0
    browser = None
    context = None
    async with async_playwright() as p:
        try:
            # --- æµè§ˆå™¨å’Œä¸Šä¸‹æ–‡çš„è®¾ç½® ---
            browser = await p.chromium.launch(headless=True)
            try:
                context = await browser.new_context(storage_state="storage_state.json")
                print("âœ… Launched browser and loaded existing storage_state.json.")
            except FileNotFoundError:
                print("âš ï¸ storage_state.json not found. Creating a new context.")
                context = await browser.new_context()
            
            page = await context.new_page()

            # --- æ ¸å¿ƒæŠ“å–é€»è¾‘ ---
            await page.goto("https://x.com", wait_until="domcontentloaded", timeout=60000)
            print(f"--- Navigated to main timeline. Goal: {DAILY_GOAL} summaries. ---")

            for i in range(TOTAL_SCROLLS):
                if processed_count >= DAILY_GOAL:
                    print(f"\nðŸŽ‰ Daily goal of {DAILY_GOAL} summaries reached. Halting script.")
                    break

                print(f"\n--- Scrolling... Round {i+1}/{TOTAL_SCROLLS} ---")
                await page.mouse.wheel(0, 8000)
                await asyncio.sleep(4)

                articles = page.locator('article[data-testid="tweet"]')
                count = await articles.count()
                print(f"  - Found {count} potential tweets on the page.")

                for i in range(count):
                    if processed_count >= DAILY_GOAL: break
                    
                    article = articles.nth(i)
                    try:
                        link_locator = article.locator("a[href*='/status/']").first
                        href = await link_locator.get_attribute('href')
                        if not href: continue
                        tweet_url = "https://x.com" + href

                        if tweet_url in PROCESSED_TWEETS: continue
                        
                        button_group = article.locator("div[role='group']")
                        like_locator = button_group.locator("button[aria-label*='Like']")
                        repost_locator = button_group.locator("button[aria-label*='Repost']")
                        like_text = await like_locator.inner_text() if await like_locator.count() > 0 else ""
                        repost_text = await repost_locator.inner_text() if await repost_locator.count() > 0 else ""
                        like_count = parse_count(like_text)
                        repost_count = parse_count(repost_text)
                        #æ–°å¢žè°ƒè¯•æ—¥å¿—
                        print(f"  - [DEBUG] Checking tweet: {tweet_url} (Likes: {like_count}, Reposts: {repost_count})")
                        if like_count > LIKE_THRESHOLD or repost_count > REPOST_THRESHOLD:
                            print(f"\n  -> Found high-value tweet: {tweet_url} (Likes: {like_count}, Reposts: {repost_count})")
                            PROCESSED_TWEETS.add(tweet_url)

                            text_locator = article.locator('[data-testid="tweetText"]')
                            original_tweet_text = await text_locator.inner_text() if await text_locator.count() > 0 else tweet_url
                            
                            if random.random() < LIKE_PROBABILITY:
                                if await like_locator.is_visible() and "Unlike" not in (await like_locator.get_attribute("aria-label")):
                                        await like_locator.click()
                                        print("  - âœ… ACTION: Liked tweet to train algorithm.")
                                        await asyncio.sleep(random.uniform(1, 3))
                                else: print("  - INFO: Tweet was already liked.")
                            else: print("  - INFO: Skipped liking due to random chance.")
                            
                            summary_text = await get_summary_from_grok_site(context, tweet_url)
                            
                            if summary_text:
                                is_added = await add_to_notion(summary_text, tweet_url, original_tweet_text)
                                if is_added:
                                    processed_count += 1
                                    print(f"  - PROGRESS: {processed_count}/{DAILY_GOAL} summaries collected.")
                            else:
                                print("  - INFO: Failed to get summary from grok.com, moving to next tweet.")
                    except Exception as e:
                        # å¿½ç•¥å¤„ç†å•æ¡æŽ¨æ–‡æ—¶çš„é”™è¯¯
                        continue
        
        # å°† except å’Œ finally ç§»åˆ°ä¸Ž try å¯¹é½çš„ä½ç½®
        except Exception as e:
            print(f"âŒ A FATAL ERROR occurred in the main process: {type(e).__name__} - {e}")

        finally:
            # --- ç¡®ä¿æœ€åŽä¿å­˜çŠ¶æ€å¹¶å…³é—­æµè§ˆå™¨ ---
            if context:
                print("\n--- Saving updated storage state... ---")
                # å¢žåŠ ä¸€ä¸ªæ£€æŸ¥ï¼Œç¡®ä¿æ–‡ä»¶å­˜åœ¨æ‰å°è¯•ä¿å­˜ï¼Œé¿å…ç¬¬ä¸€æ¬¡è¿è¡Œå‡ºé”™
                if os.path.exists("storage_state.json"):
                    await context.storage_state(path="storage_state.json")
                    print("--- Updated storage state saved to storage_state.json. ---")
            if browser:
                await browser.close()
                print("--- Browser closed. ---")

        print(f"\n--- Script finished. Total summaries collected: {processed_count} ---")



if __name__ == "__main__":
    notion = Client(auth=NOTION_API_KEY)
    asyncio.run(scrape_main_timeline())